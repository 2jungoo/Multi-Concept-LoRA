"""
ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì‚¬
- ë¦¬ì‚¬ì´ì§• ë° í¬ë¡­
- BLIP ìë™ ìº¡ì…”ë‹
- íŠ¸ë¦¬ê±° ì›Œë“œ ì‚½ì…
"""

import os
import shutil
import json
from pathlib import Path
from PIL import Image, ImageFilter
import cv2
import numpy as np
from tqdm import tqdm
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
import warnings
warnings.filterwarnings('ignore')


class DataPreprocessor:
    """ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, output_size=512, min_quality_score=100):
        self.output_size = output_size
        self.min_quality_score = min_quality_score
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # BLIP ëª¨ë¸ ë¡œë“œ
        print("Loading BLIP model for auto-captioning...")
        self.blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
        self.blip_model = BlipForConditionalGeneration.from_pretrained(
            "Salesforce/blip-image-captioning-large",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
        ).to(self.device)
        print(f"BLIP model loaded on {self.device}")
        
        # íŠ¸ë¦¬ê±° ì›Œë“œ ì„¤ì •
        self.trigger_words = {
            'anime': 'anistyle',
            'watercolor': 'wcstyle', 
            'cartoon': 'cartoonstyle',
            'pixel': 'pixstyle',
            'face': 'sksperson'
        }
    
    def calculate_sharpness(self, image_path: str) -> float:
        """Laplacian varianceë¡œ ì´ë¯¸ì§€ ì„ ëª…ë„ ê³„ì‚°"""
        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            return 0
        laplacian_var = cv2.Laplacian(img, cv2.CV_64F).var()
        return laplacian_var
    
    def check_image_quality(self, image_path: str) -> dict:
        """ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì‚¬"""
        try:
            img = Image.open(image_path)
            width, height = img.size
            
            # ì„ ëª…ë„ ê³„ì‚°
            sharpness = self.calculate_sharpness(image_path)
            
            # í•´ìƒë„ ì²´í¬ (60x60ì€ ë„ˆë¬´ ì‘ìŒ)
            min_dimension = min(width, height)
            is_too_small = min_dimension < 256  # 256 ë¯¸ë§Œì€ ê²½ê³ 
            is_very_small = min_dimension < 128  # 128 ë¯¸ë§Œì€ ì‚¬ìš© ë¶ˆê°€
            
            return {
                'path': image_path,
                'width': width,
                'height': height,
                'sharpness': sharpness,
                'is_sharp_enough': sharpness >= self.min_quality_score,
                'is_too_small': is_too_small,
                'is_very_small': is_very_small,
                'is_valid': not is_very_small,
                'mode': img.mode
            }
        except Exception as e:
            return {
                'path': image_path,
                'error': str(e),
                'is_valid': False
            }
    
    def resize_and_crop(self, img: Image.Image, target_size: int = 512) -> Image.Image:
        """ì¤‘ì•™ í¬ë¡­ í›„ ë¦¬ì‚¬ì´ì§•"""
        # RGBë¡œ ë³€í™˜
        if img.mode != 'RGB':
            img = img.convert('RGB')
        
        width, height = img.size
        
        # ì •ì‚¬ê°í˜• í¬ë¡­
        crop_size = min(width, height)
        left = (width - crop_size) // 2
        top = (height - crop_size) // 2
        img = img.crop((left, top, left + crop_size, top + crop_size))
        
        # ë¦¬ì‚¬ì´ì§• (ì‘ì€ ì´ë¯¸ì§€ëŠ” LANCZOS ì—…ìŠ¤ì¼€ì¼)
        img = img.resize((target_size, target_size), Image.LANCZOS)
        
        return img
    
    def generate_caption(self, image: Image.Image, style_hint: str = "") -> str:
        """BLIPì„ ì‚¬ìš©í•œ ìë™ ìº¡ì…˜ ìƒì„±"""
        # ì¡°ê±´ë¶€ ìº¡ì…˜ ìƒì„± (ìŠ¤íƒ€ì¼ íŒíŠ¸ ì œê³µ)
        if style_hint:
            prompt = f"a {style_hint}"
        else:
            prompt = "a"
        
        inputs = self.blip_processor(image, text=prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            out = self.blip_model.generate(
                **inputs,
                max_length=75,
                num_beams=3,
                do_sample=True,
                top_p=0.9,
                temperature=0.7
            )
        
        caption = self.blip_processor.decode(out[0], skip_special_tokens=True)
        return caption
    
    def process_dataset(
        self,
        input_folder: str,
        output_folder: str,
        style_name: str,
        max_images: int = 100,
        use_manual_captions: bool = False,
        manual_caption_template: str = None
    ):
        """ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        
        input_path = Path(input_folder)
        output_path = Path(output_folder)
        output_path.mkdir(parents=True, exist_ok=True)
        
        trigger_word = self.trigger_words.get(style_name, style_name)
        
        # ì§€ì›í•˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ì
        valid_extensions = {'.png', '.jpg', '.jpeg', '.webp', '.bmp'}
        
        # ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘
        image_files = [
            f for f in input_path.iterdir()
            if f.suffix.lower() in valid_extensions
        ]
        
        print(f"\n{'='*60}")
        print(f"Processing {style_name} dataset")
        print(f"Found {len(image_files)} images in {input_folder}")
        print(f"Trigger word: {trigger_word}")
        print(f"{'='*60}")
        
        # í’ˆì§ˆ ê²€ì‚¬
        print("\n[1/3] Checking image quality...")
        quality_results = []
        for img_file in tqdm(image_files, desc="Quality check"):
            result = self.check_image_quality(str(img_file))
            quality_results.append(result)
        
        # í’ˆì§ˆ í†µê³„
        valid_images = [r for r in quality_results if r.get('is_valid', False)]
        small_images = [r for r in quality_results if r.get('is_too_small', False)]
        very_small_images = [r for r in quality_results if r.get('is_very_small', False)]
        
        print(f"\nğŸ“Š Quality Report:")
        print(f"   - Total images: {len(image_files)}")
        print(f"   - Valid for training: {len(valid_images)}")
        print(f"   - Small (< 256px, will upscale): {len(small_images)}")
        print(f"   - Too small (< 128px, skipped): {len(very_small_images)}")
        
        if very_small_images:
            print(f"\nâš ï¸  WARNING: {len(very_small_images)} images are too small!")
            print("   Consider replacing these with higher resolution images.")
            for r in very_small_images[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                print(f"   - {Path(r['path']).name}: {r.get('width', 'N/A')}x{r.get('height', 'N/A')}")
        
        # ì²˜ë¦¬í•  ì´ë¯¸ì§€ ì„ íƒ
        images_to_process = [r for r in valid_images][:max_images]
        
        # ì²˜ë¦¬ ë° ìº¡ì…”ë‹
        print(f"\n[2/3] Processing and captioning {len(images_to_process)} images...")
        processed_count = 0
        caption_data = []
        
        for idx, quality_info in enumerate(tqdm(images_to_process, desc="Processing")):
            try:
                img_path = Path(quality_info['path'])
                img = Image.open(img_path)
                
                # ë¦¬ì‚¬ì´ì§•
                processed_img = self.resize_and_crop(img, self.output_size)
                
                # ìƒˆ íŒŒì¼ëª…
                new_filename = f"{style_name}_{idx:04d}.png"
                output_img_path = output_path / new_filename
                processed_img.save(output_img_path, 'PNG', quality=95)
                
                # ìº¡ì…˜ ìƒì„±
                if use_manual_captions and manual_caption_template:
                    caption = manual_caption_template
                else:
                    style_hints = {
                        'anime': 'anime style illustration',
                        'watercolor': 'watercolor painting',
                        'cartoon': 'cartoon style drawing',
                        'pixel': 'pixel art'
                    }
                    base_caption = self.generate_caption(
                        processed_img, 
                        style_hints.get(style_name, '')
                    )
                    caption = f"{trigger_word}, {base_caption}"
                
                # ìº¡ì…˜ íŒŒì¼ ì €ì¥
                caption_file_path = output_path / f"{style_name}_{idx:04d}.txt"
                with open(caption_file_path, 'w', encoding='utf-8') as f:
                    f.write(caption)
                
                caption_data.append({
                    'image': new_filename,
                    'caption': caption,
                    'original_path': str(img_path),
                    'original_size': f"{quality_info['width']}x{quality_info['height']}"
                })
                
                processed_count += 1
                
            except Exception as e:
                print(f"\nâŒ Error processing {img_path}: {e}")
                continue
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'style': style_name,
            'trigger_word': trigger_word,
            'total_processed': processed_count,
            'output_resolution': self.output_size,
            'captions': caption_data
        }
        
        metadata_path = output_path / 'metadata.json'
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… Dataset processing complete!")
        print(f"   - Processed images: {processed_count}")
        print(f"   - Output folder: {output_path}")
        print(f"   - Metadata saved: {metadata_path}")
        
        return metadata


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
    PROJECT_ROOT = Path("/home/claude/lora_project")
    
    # ì›ë³¸ ë°ì´í„°ì…‹ ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)
    RAW_DATA_PATHS = {
        'anime': '/path/to/your/anime/images',       # âš ï¸ ìˆ˜ì • í•„ìš”
        'watercolor': '/path/to/your/watercolor/images',  # âš ï¸ ìˆ˜ì • í•„ìš”
        'cartoon': '/path/to/your/cartoon/images'    # âš ï¸ ìˆ˜ì • í•„ìš”
    }
    
    # ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ì €ì¥ ê²½ë¡œ
    PROCESSED_DATA_PATHS = {
        'anime': PROJECT_ROOT / 'data' / 'anime',
        'watercolor': PROJECT_ROOT / 'data' / 'watercolor',
        'cartoon': PROJECT_ROOT / 'data' / 'cartoon'
    }
    
    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = DataPreprocessor(output_size=512, min_quality_score=50)
    
    # ê° ìŠ¤íƒ€ì¼ë³„ ì²˜ë¦¬
    for style_name, raw_path in RAW_DATA_PATHS.items():
        if not Path(raw_path).exists():
            print(f"\nâš ï¸  Skipping {style_name}: Path does not exist: {raw_path}")
            continue
        
        preprocessor.process_dataset(
            input_folder=raw_path,
            output_folder=str(PROCESSED_DATA_PATHS[style_name]),
            style_name=style_name,
            max_images=100  # ê° ìŠ¤íƒ€ì¼ë‹¹ ìµœëŒ€ 100ì¥
        )
    
    print("\n" + "="*60)
    print("All datasets processed!")
    print("="*60)


if __name__ == "__main__":
    main()
